# LLM Configuration
# Choose provider: "gemini" (default), "gemini_aiplatform", or "cursor"
LLM_PROVIDER=gemini

# Gemini Settings (for both gemini and gemini_aiplatform providers)
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-2.0-flash

# Gemini AI Platform specific (when LLM_PROVIDER=gemini_aiplatform)
# Uncomment and set if using AI Platform with character limits
# LLM_MAX_INPUT_CHARS=1000000

# Rate limiting for Gemini (default: 3 retries)
# LLM_RATE_LIMIT_MAX_RETRIES=3

# Cursor CLI Settings (when LLM_PROVIDER=cursor)
# CURSOR_MODEL=your_preferred_model
# CURSOR_TIMEOUT=120
# CURSOR_API_KEY=your_cursor_api_key_for_headless_mode

# GitHub Access
# Optional: Increases rate limits and enables private repo access
# GITHUB_TOKEN=your_github_personal_access_token

# File Processing Configuration
# Number of files processed per chunk during summarization (50-5000)
LLM_FILE_SUMMARY_CHUNK_SIZE=1000

# Maximum number of representative files to analyze (50-2000)  
LLM_FILE_SUMMARY_MAX_FILES=400

# API Configuration (for FastAPI backend)
# Job retention time in seconds (default: 86400 = 24 hours)
# JOB_RETENTION_SECONDS=86400

# Webhook Configuration (for async job notifications)
# Secret for webhook payload signing (HMAC-SHA256)
# WEBHOOK_SECRET=your_webhook_secret_key